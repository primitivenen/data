{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/current/spark2-client')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import lit, col, instr, expr, pow, round, bround, corr, count, mean, stddev_pop, min, max\n",
    "from pyspark.sql.functions import monotonically_increasing_id, initcap, lower, upper, ltrim, rtrim, rpad, lpad, trim\n",
    "from pyspark.sql.functions import regexp_replace, translate, regexp_extract, current_date, current_timestamp, struct\n",
    "from pyspark.sql.functions import date_add, date_sub, datediff, months_between, to_date, to_timestamp, coalesce, split, size\n",
    "from pyspark.sql.functions import array_contains, explode, udf\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Spark():\n",
    "\n",
    "    conf = pyspark.SparkConf().setAll([\n",
    "        ('spark.submit.deployMode', 'client'), # deploy in yarn-client or yarn-cluster\n",
    "        ('spark.executor.memory', '8g'),       # memory allocated for each executor\n",
    "        ('spark.executor.cores', '3'),         # number of cores for each executor\n",
    "        ('spark.executor.instances', '10'),    # number of executors in total\n",
    "        ('spark.yarn.am.memory', '24g')])      # memory for spark driver (application master)\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(conf = conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = get_Spark()\n",
    "spark_context = spark.sparkContext\n",
    "hc = HiveContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import datetime\n",
    "start_time = datetime.datetime.combine(d1, datetime.time(0, 0))\n",
    "print (start_time)\n",
    "\n",
    "def hive2pandas(hc, query):\n",
    "    spark_df = hc.sql(\"\"\"{}\"\"\".format(query))\n",
    "    # Convert to pandas dataframe\n",
    "    df = spark_df.toPandas()\n",
    "    return df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def addLocation(hc, p):\n",
    "    #query = \"\"\"SELECT distinct ROUND(CAST(start_loc_lat as float), 3) as loc_lat, ROUND(CAST(start_loc_lon as float), 3) as loc_lon from ubi.conv_trips_complete union SELECT distinct ROUND(CAST(end_loc_lat as float), 3) as loc_lat, ROUND(CAST(end_loc_lon as float), 3) as loc_lon from ubi.conv_trips_complete where start_day>='{0}' and start_day<='{1}'\"\"\".format(d1, d2)\n",
    "    query = \"\"\"SELECT distinct CAST(start_loc_lat as float) as loc_lat, CAST(start_loc_lon as float) as loc_lon from ubi.conv_trips_complete union SELECT distinct CAST(end_loc_lat as float) as loc_lat, CAST(end_loc_lon as float) as loc_lon from ubi.conv_trips_complete where start_day>='{0}' and start_day<='{1}'\"\"\".format(d1, d2)\n",
    "    print(query)\n",
    "    df=hive2pandas(hc, query)\n",
    "    with open('/home/wchen/ubi2/latlon2location_' + str(p) + '.txt', 'w') as outfile:\n",
    "        for x in range(len(df)):\n",
    "            if(int(df.iloc[x,0] * 10) % 10 == p):\n",
    "                g = geocoder.gaode([df.iloc[x,0], df.iloc[x,1]], method='reverse', key='27522a3d9da8f4e80d6580c80d010d4c')\n",
    "                adds = g.address\n",
    "                print(adds)\n",
    "                if g.address is None:\n",
    "                    adds=''\n",
    "                elif type(g.address)==list:\n",
    "                    adds=str(adds)\n",
    "                outfile.write('%.3f'%(df.iloc[x,0]) + '\\t' + '%.3f'%(df.iloc[x,1]) + '\\t' + adds.encode('utf-8') + '\\n')\n",
    "    print('Done with location: ' + str(p))\n",
    "    \n",
    "    addLocation(hc,0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile='/home/wchen/dsc/file/combined.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.isfile(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(infile, sep=',',  dtype={'loc_latitude': float, 'loc_longtitude':float, 'address':str}, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geocoder\n",
    "\n",
    "for x in range(len(df)):\n",
    "    if pd.isna(df.iloc[x,2]):\n",
    "        #print df.iloc[x]\n",
    "        g = geocoder.gaode([df.iloc[x,0], df.iloc[x,1]], method='reverse', key='27522a3d9da8f4e80d6580c80d010d4c')\n",
    "        adds = g.address\n",
    "        if g.address is None:\n",
    "            adds=''\n",
    "        elif type(g.address)==list:\n",
    "            adds=str(adds)\n",
    "        df.iloc[x,2] = adds.encode('utf-8')\n",
    "        #print df.iloc[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "\n",
    "for x in range(len(df)):\n",
    "    if pd.isna(df.iloc[x,2]):\n",
    "        print df.iloc[x]\n",
    "print(\"Check completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([StructField(\"loc_latitude\", DoubleType(), True)\\\n",
    "               ,StructField(\"loc_longtitude\", DoubleType(), True)\\\n",
    "               ,StructField(\"address\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['address'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "pd.to_numeric(df['loc_latitude'])\n",
    "pd.to_numeric(df['loc_longtitude'])\n",
    "df['address'] = str(df['address']).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_latitude</th>\n",
       "      <th>loc_longtitude</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.089</td>\n",
       "      <td>113.280</td>\n",
       "      <td>广东省广州市海珠区昌岗街道细岗路106号晓园北小区</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.011</td>\n",
       "      <td>121.526</td>\n",
       "      <td>上海市闵行区浦江镇丰南路1270号</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.040</td>\n",
       "      <td>113.354</td>\n",
       "      <td>广东省韶关市乐昌市长来镇G4W3乐广高速</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.009</td>\n",
       "      <td>113.318</td>\n",
       "      <td>广东省广州市番禺区大石街道礼村中路56号主人花园</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.065</td>\n",
       "      <td>114.203</td>\n",
       "      <td>河北省石家庄市井陉县上安镇李志刚诊所</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loc_latitude  loc_longtitude                    address\n",
       "0        23.089         113.280  广东省广州市海珠区昌岗街道细岗路106号晓园北小区\n",
       "1        31.011         121.526          上海市闵行区浦江镇丰南路1270号\n",
       "2        25.040         113.354       广东省韶关市乐昌市长来镇G4W3乐广高速\n",
       "3        23.009         113.318   广东省广州市番禺区大石街道礼村中路56号主人花园\n",
       "4        38.065         114.203         河北省石家庄市井陉县上安镇李志刚诊所"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.to_csv(\"df_temp.csv.gz\", index=False, compression=\"gzip\")\n",
    "import pandas as pd\n",
    "df_2 = pd.read_csv(\"df_temp.csv.gz\", encoding=\"utf-8\")\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols=[\"loc_latitude\",\"loc_longtitude\",\"address\"]\n",
    "#spark_df = hc.createDataFrame([tuple(x for x in record.tolist()) for record in df.to_records(index=False)], df.columns.tolist())\n",
    "spark_df = hc.createDataFrame(df[my_cols], mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO TABLE ubi.address SELECT loc_latitude, loc_longtitude, address FROM update_dataframe\n",
      "Table address creation done.\n"
     ]
    }
   ],
   "source": [
    "cols = [when(~col(x).isin(\"NULL\", \"NA\", \"NaN\",\"\"), col(x)).alias(x) for x in spark_df.columns]\n",
    "spark_df = spark_df.select(*cols)\n",
    "spark_df.registerTempTable('update_dataframe')\n",
    "sql_cmd = \"\"\"INSERT INTO TABLE ubi.address SELECT loc_latitude, loc_longtitude, address FROM update_dataframe\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd)\n",
    "print('Table address creation done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
