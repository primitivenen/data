{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/current/spark2-client')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import lit, col, instr, expr, pow, round, bround, corr, count, mean, stddev_pop, min, max\n",
    "from pyspark.sql.functions import monotonically_increasing_id, initcap, lower, upper, ltrim, rtrim, rpad, lpad, trim\n",
    "from pyspark.sql.functions import regexp_replace, translate, regexp_extract, current_date, current_timestamp, struct\n",
    "from pyspark.sql.functions import date_add, date_sub, datediff, months_between, to_date, to_timestamp, coalesce, split, size\n",
    "from pyspark.sql.functions import array_contains, explode, udf\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Spark():\n",
    "\n",
    "    conf = pyspark.SparkConf().setAll([\n",
    "        ('spark.submit.deployMode', 'client'), # deploy in yarn-client or yarn-cluster\n",
    "        ('spark.executor.memory', '8g'),       # memory allocated for each executor\n",
    "        ('spark.executor.cores', '3'),         # number of cores for each executor\n",
    "        ('spark.executor.instances', '10'),    # number of executors in total\n",
    "        ('spark.yarn.am.memory', '10g')])      # memory for spark driver (application master)\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"name\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(conf = conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = get_Spark()\n",
    "spark_context = spark.sparkContext\n",
    "hc = HiveContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltimeFormat = \"yyyyMMddHHmmss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_schema(model):\n",
    "    types = {\n",
    "        \"string\": StringType(),\n",
    "        \"long\": LongType(),\n",
    "        \"date\": StringType(),\n",
    "        \"categorical - integer\": IntegerType(),\n",
    "        \"double\": DoubleType(),\n",
    "        \"integer\": DoubleType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"float\": FloatType()\n",
    "    }\n",
    "\n",
    "    with open('schemas/{}_schema.csv'.format(model if model != \"conv\" else model + \"_1s\"), 'r') as lines:\n",
    "        columns = []\n",
    "        for line in lines:\n",
    "            lineArray = line.split(\",\")\n",
    "            columns.append(StructField(lineArray[1], types[lineArray[2]]))\n",
    "        schema = StructType(columns)\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "d1 = date(2019,1,31)\n",
    "d2 = date(2019,2,2)\n",
    "delta = d2 - d1\n",
    "days=[]\n",
    "for i in range(delta.days+1):\n",
    "    days.append((d1+timedelta(i)).strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def run_cmd(args_list):\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    proc.communicate()\n",
    "    return proc.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load1sDataPerDay(day, dfSchema):\n",
    "    data_file = \"hdfs://namenode:8020/apps/hive/warehouse/conv_tsp_tbls.db/a7m_1s_orc/dt=\" + str(day)\n",
    "    returncode = run_cmd(['hdfs', 'dfs', '-test', '-e', data_file])\n",
    "    if returncode:\n",
    "        print('{} does not exist, skipping ..'.format(data_file))\n",
    "    else :\n",
    "        return spark.read.format(\"orc\").schema(dfSchema).load(data_file)#.where(\"vin='LMGMS1G85J1024832'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load5sDataPerDay(day):\n",
    "    data_file = \"hdfs://namenode:8020/apps/hive/warehouse/conv_tsp_tbls.db/a7m_5s_orc/dt=\" + str(day) + \"/000000_0\"\n",
    "    returncode = run_cmd(['hdfs', 'dfs', '-test', '-e', data_file])\n",
    "    if returncode:\n",
    "        print('{} does not exist, skipping ..'.format(data_file))\n",
    "    else :\n",
    "        return spark.read.format(\"orc\").load(data_file).select(\"_col0\", \"_col1\", \"_col19\")\\\n",
    "            .withColumnRenamed(\"_col0\", \"vin\").withColumnRenamed(\"_col1\", \"normaltime\")\\\n",
    "            .withColumnRenamed(\"_col19\", \"icm_totalodometer\")#.where(\"vin='LMGMS1G85J1024832'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309156 starting/ending rows  ..\n",
      "20190131 processing ..\n",
      "309156 rows loaded ..\n",
      "266963 starting/ending rows  ..\n",
      "20190201 processing ..\n",
      "576119 rows loaded ..\n",
      "hdfs://namenode:8020/apps/hive/warehouse/conv_tsp_tbls.db/a7m_1s_orc/dt=20190202 does not exist, skipping ..\n",
      "hdfs://namenode:8020/apps/hive/warehouse/conv_tsp_tbls.db/a7m_5s_orc/dt=20190202/000000_0 does not exist, skipping ..\n",
      "20190202 processing ..\n",
      "576119 rows loaded ..\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "my_window = Window.partitionBy(\"vin\").orderBy(\"normaltime\")\n",
    "my_next_window = Window.partitionBy(\"vin\").orderBy(desc(\"normaltime\"))\n",
    "\n",
    "dfSchema = build_schema(\"conv\")\n",
    "df = None\n",
    "for day in days:\n",
    "    df_1s = load1sDataPerDay(day, dfSchema)\n",
    "    df_5s = load5sDataPerDay(day)\n",
    "\n",
    "    if not (df_1s is None or df_5s is None):\n",
    "        df_tmp = df_1s.join(df_5s, [\"vin\", \"normaltime\"], \"inner\").withColumn(\"normaltime\", to_timestamp(col(\"normaltime\"), normaltimeFormat))\n",
    "        df_tmp = df_tmp.where(\"tel_latitudedeg > 0 and tel_longitudedeg > 0\")\n",
    "        df_tmp = df_tmp.withColumn(\"next_normaltime\", F.lag(df_tmp.normaltime).over(my_next_window))\n",
    "        df_tmp = df_tmp.withColumn(\"prev_normaltime\", F.lag(df_tmp.normaltime).over(my_window))\n",
    "        df_tmp = df_tmp.withColumn(\"prev_diff\", F.when(F.isnull(df_tmp.normaltime.cast(\"long\") - df_tmp.prev_normaltime.cast(\"long\")), 1000).otherwise(df_tmp.normaltime.cast(\"long\") - df_tmp.prev_normaltime.cast(\"long\")))\n",
    "        df_tmp = df_tmp.withColumn(\"next_diff\", F.when(F.isnull(df_tmp.next_normaltime.cast(\"long\") - df_tmp.normaltime.cast(\"long\")), 1000).otherwise(df_tmp.next_normaltime.cast(\"long\") - df_tmp.normaltime.cast(\"long\")))\n",
    "        df_tmp = df_tmp.where(\"prev_diff >= 60 or next_diff >= 60\")\n",
    "        print('{} starting/ending rows  ..'.format(df_tmp.count()))\n",
    "        if df is None:\n",
    "            df = df_tmp\n",
    "        else:\n",
    "            df = df.union(df_tmp)\n",
    "    print('{} processing ..'.format(day))\n",
    "    if not (df is None):\n",
    "        print('{} rows loaded ..'.format(df.count()))\n",
    "#if not (df is None):\n",
    "#    df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.where(\"vin='LMGMS1G87J1023682'\").select(\"vin\", \"prev_diff\", \"next_diff\", \"normaltime\", \"prev_normaltime\", \"next_normaltime\", \"icm_totalodometer\", \"tel_latitudedeg\")\n",
    "#df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "res['vin'] = []\n",
    "res['start_time'] = []\n",
    "res['end_time'] = []\n",
    "res['start_lat'] = []\n",
    "res['start_lon'] = []\n",
    "res['end_lat'] = []\n",
    "res['end_lon'] = []\n",
    "res['distance']  = []\n",
    "\n",
    "#x = df.select(\"vin\", \"prev_diff\", \"next_diff\", \"normaltime\", \"prev_normaltime\", \"next_normaltime\", \"icm_totalodometer\", \"tel_latitudedeg\")\n",
    "#x.show(20)\n",
    "\n",
    "df = df.sort([\"vin\",\"normaltime\"], ascending=[0,1])\n",
    "pdf = df.toPandas()\n",
    "\n",
    "#pdf.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices where time difference longer than threshold\n",
    "indices = pdf.index[(pdf['prev_diff'] >= int(60)) | (pdf['next_diff'] >= int(60))].tolist()\n",
    "\n",
    "for i in range(len(indices) - 1):\n",
    "    lo = indices[i]\n",
    "    hi = indices[i+1]\n",
    "\n",
    "    if pdf['vin'].iloc[lo] != pdf['vin'].iloc[hi]:\n",
    "        #print ('{} finished'.format(pdf['vin'].iloc[lo]))\n",
    "        continue\n",
    "        \n",
    "    if pdf['prev_diff'].iloc[lo] < int(60) or pdf['next_diff'].iloc[hi] < int(60) : \n",
    "        continue\n",
    "    \n",
    "    if pdf['icm_totalodometer'].iloc[hi] - pdf['icm_totalodometer'].iloc[lo] < 1 and int((pdf['normaltime'].iloc[hi] - pdf['normaltime'].iloc[lo]).total_seconds()) < 600 :\n",
    "        continue\n",
    "    \n",
    "    res['vin'].append(pdf['vin'].iloc[lo])\n",
    "    res['start_time'].append(pdf['normaltime'].iloc[lo])\n",
    "    res['start_lat'].append(pdf['tel_latitudedeg'].iloc[lo] + pdf['tel_latitudemin'].iloc[lo] / 60.0 \\\n",
    "                               + pdf['tel_latitudesec'].iloc[lo] / 3600.0)\n",
    "    res['start_lon'].append(pdf['tel_longitudedeg'].iloc[lo] + pdf['tel_longitudemin'].iloc[lo] / 60.0 \\\n",
    "                               + pdf['tel_longitudesec'].iloc[lo] / 3600.0)\n",
    "    res['end_time'].append(pdf['normaltime'].iloc[hi])\n",
    "    res['end_lat'].append(pdf['tel_latitudedeg'].iloc[hi] + pdf['tel_latitudemin'].iloc[hi] / 60.0 \\\n",
    "                               + pdf['tel_latitudesec'].iloc[hi] / 3600.0)\n",
    "    res['end_lon'].append(pdf['tel_longitudedeg'].iloc[hi] + pdf['tel_longitudemin'].iloc[hi] / 60.0 \\\n",
    "                               + pdf['tel_longitudesec'].iloc[hi] / 3600.0)\n",
    "    res['distance'].append(pdf['icm_totalodometer'].iloc[hi] - pdf['icm_totalodometer'].iloc[lo])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "res_df = pd.DataFrame(res)\n",
    "\n",
    "res_df['duration'] = res_df.apply(lambda x: int((x['end_time']-x['start_time']).total_seconds())/3600.0, axis=1)\n",
    "res_df['speed'] = res_df.apply(lambda x: float(x['distance'] / x['duration']), axis=1)\n",
    "\n",
    "res_df.head(100)\n",
    "res_df = res_df[res_df.duration<=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import time\n",
    "firstDay = time.mktime(d1.timetuple())\n",
    "print (firstDay)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_col = [\"vin\"\n",
    "               ,\"start_time\"\n",
    "               ,\"start_lat\"\n",
    "               ,\"start_lon\"\n",
    "               ,\"end_time\"\n",
    "               ,\"end_lat\"\n",
    "               ,\"end_lon\"\n",
    "               ,\"distance\"\n",
    "               ,\"duration\"\n",
    "               ,\"speed\"]\n",
    "\n",
    "mySchema = StructType([StructField(\"vin\", StringType(), True)\\\n",
    "               ,StructField(\"start_time\", StringType(), True)\\\n",
    "               ,StructField(\"start_lat\", DoubleType(), True)\\\n",
    "               ,StructField(\"start_lon\", DoubleType(), True)\\\n",
    "               ,StructField(\"end_time\", StringType(), True)\\\n",
    "               ,StructField(\"end_lat\", DoubleType(), True)\\\n",
    "               ,StructField(\"end_lon\", DoubleType(), True)\\\n",
    "               ,StructField(\"distance\", DoubleType(), True)\\\n",
    "               ,StructField(\"duration\", DoubleType(), True)\\\n",
    "               ,StructField(\"speed\", DoubleType(), True)])\n",
    "spark_df = hc.createDataFrame(res_df[my_col], mySchema)\n",
    "cols = [when(~col(x).isin(\"NULL\", \"NA\", \"NaN\",\"\"), col(x)).alias(x) for x in spark_df.columns]\n",
    "spark_df = spark_df.select(*cols)\n",
    "spark_df.registerTempTable('update_dataframe')\n",
    "\n",
    "sql_cmd = \"\"\"INSERT INTO TABLE ubi.conv_trips SELECT * from update_dataframe\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd)\n",
    "print('Table address creation done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import datetime\n",
    "start_time = datetime.datetime.combine(d1, datetime.time(0, 0))\n",
    "print (start_time)\n",
    "\n",
    "def hive2pandas(hc, query):\n",
    "    spark_df = hc.sql(\"\"\"{}\"\"\".format(query))\n",
    "    # Convert to pandas dataframe\n",
    "    df = spark_df.toPandas()\n",
    "    return df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def addLocation(hc, p):\n",
    "    #query = \"\"\"SELECT distinct ROUND(CAST(start_loc_lat as float), 3) as loc_lat, ROUND(CAST(start_loc_lon as float), 3) as loc_lon from ubi.conv_trips_complete union SELECT distinct ROUND(CAST(end_loc_lat as float), 3) as loc_lat, ROUND(CAST(end_loc_lon as float), 3) as loc_lon from ubi.conv_trips_complete where start_day>='{0}' and start_day<='{1}'\"\"\".format(d1, d2)\n",
    "    query = \"\"\"SELECT distinct CAST(start_loc_lat as float) as loc_lat, CAST(start_loc_lon as float) as loc_lon from ubi.conv_trips_complete union SELECT distinct CAST(end_loc_lat as float) as loc_lat, CAST(end_loc_lon as float) as loc_lon from ubi.conv_trips_complete where start_day>='{0}' and start_day<='{1}'\"\"\".format(d1, d2)\n",
    "    print(query)\n",
    "    df=hive2pandas(hc, query)\n",
    "    with open('/home/wchen/ubi2/latlon2location_' + str(p) + '.txt', 'w') as outfile:\n",
    "        for x in range(len(df)):\n",
    "            if(int(df.iloc[x,0] * 10) % 10 == p):\n",
    "                g = geocoder.gaode([df.iloc[x,0], df.iloc[x,1]], method='reverse', key='27522a3d9da8f4e80d6580c80d010d4c')\n",
    "                adds = g.address\n",
    "                print(adds)\n",
    "                if g.address is None:\n",
    "                    adds=''\n",
    "                elif type(g.address)==list:\n",
    "                    adds=str(adds)\n",
    "                outfile.write('%.3f'%(df.iloc[x,0]) + '\\t' + '%.3f'%(df.iloc[x,1]) + '\\t' + adds.encode('utf-8') + '\\n')\n",
    "    print('Done with location: ' + str(p))\n",
    "    \n",
    "    addLocation(hc,0)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
