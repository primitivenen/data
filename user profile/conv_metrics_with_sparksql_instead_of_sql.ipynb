{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/current/spark2-client')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import lit, col, instr, expr, pow, round, bround, corr, count, mean, stddev_pop, min, max\n",
    "from pyspark.sql.functions import monotonically_increasing_id, initcap, lower, upper, ltrim, rtrim, rpad, lpad, trim\n",
    "from pyspark.sql.functions import regexp_replace, translate, regexp_extract, current_date, current_timestamp, struct\n",
    "from pyspark.sql.functions import date_add, date_sub, datediff, months_between, to_date, to_timestamp, coalesce, split, size\n",
    "from pyspark.sql.functions import array_contains, explode, udf\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType, LongType\n",
    "\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Spark():\n",
    "\n",
    "    conf = pyspark.SparkConf().setAll([\n",
    "        ('spark.submit.deployMode', 'client'), # deploy in yarn-client or yarn-cluster\n",
    "        ('spark.executor.memory', '8g'),       # memory allocated for each executor\n",
    "        ('spark.executor.cores', '3'),         # number of cores for each executor\n",
    "        ('spark.executor.instances', '10'),    # number of executors in total\n",
    "        ('spark.yarn.am.memory', '10g')])      # memory for spark driver (application master)\n",
    "    spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"metric_generation\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(conf = conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = get_Spark()\n",
    "spark_context = spark.sparkContext\n",
    "hc = HiveContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hive2spark(hc, query):\n",
    "    spark_df = hc.sql(\"\"\"{}\"\"\".format(query))\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select vin, start_time, start_day, end_time, \\\n",
    "    from_unixtime(unix_timestamp(CONCAT(start_day, ' ', '07:00:00'), 'yyyyMMdd HH:mm:ss')) as peak_start, \\\n",
    "    from_unixtime(unix_timestamp(CONCAT(start_day, ' ', '09:00:00'), 'yyyyMMdd HH:mm:ss')) as peak_end \\\n",
    "    from ubi.conv_trips_complete\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = hive2spark(hc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vin=u'LMGMS1G80H1000254', start_time=u'2018-06-29 06:37:51', start_day=u'20180629', end_time=u'2018-06-29 06:47:22', peak_start=u'2018-06-29 07:00:00', peak_end=u'2018-06-29 09:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 07:00:00|2018-06-29 09:00:00|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 07:00:00|2018-07-07 09:00:00|\n",
      "|LMGMS1G80J1009719|2018-08-14 02:18:32| 20180814|2018-08-14 02:24:00|2018-08-14 07:00:00|2018-08-14 09:00:00|\n",
      "|LMGMS1G80J1015701|2019-01-12 01:15:52| 20190112|2019-01-12 01:39:32|2019-01-12 07:00:00|2019-01-12 09:00:00|\n",
      "|LMGMS1G80J1017061|2019-02-05 01:53:03| 20190205|2019-02-05 02:13:11|2019-02-05 07:00:00|2019-02-05 09:00:00|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vin: string, start_time: string, start_day: string, end_time: timestamp, peak_start: string, peak_end: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.sql import functions as F\n",
    "normaltimeFormat = \"yyyyMMddHHmmss\"\n",
    "df1.withColumn(\"start_time\",df1.start_time.astype('Timestamp'))\n",
    "df1.withColumn(\"end_time\",df1.end_time.astype('Timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df1.start_day.astype('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 07:00:00|2018-06-29 09:00:00|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 07:00:00|2018-07-07 09:00:00|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_1=F.when(df1.start_time > df1.peak_start,df1.start_time.astype('Timestamp'))\\\n",
    ".otherwise(df1.peak_start.astype('Timestamp'))\n",
    "new_column_2=F.when(df1.end_time < df1.peak_end,df1.end_time.astype('Timestamp'))\\\n",
    ".otherwise(df1.peak_end.astype('Timestamp'))\n",
    "df1=df1.withColumn(\"peak_start_time\",new_column_1)\n",
    "df1=df1.withColumn(\"peak_end_time\",new_column_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|    peak_start_time|      peak_end_time|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 07:00:00|2018-06-29 09:00:00|2018-06-29 07:00:00|2018-06-29 06:47:22|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 07:00:00|2018-07-07 09:00:00|2018-07-07 07:00:00|2018-07-07 01:19:43|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_3 = (to_timestamp(df1.end_time.astype('Timestamp'),normaltimeFormat).cast(\"long\")\n",
    "                -to_timestamp(df1.start_time.astype('Timestamp'),normaltimeFormat).cast(\"long\"))/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|    peak_start_time|      peak_end_time|      driving_mins|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 07:00:00|2018-06-29 09:00:00|2018-06-29 07:00:00|2018-06-29 06:47:22| 9.516666666666667|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 07:00:00|2018-07-07 09:00:00|2018-07-07 07:00:00|2018-07-07 01:19:43|19.983333333333334|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumn(\"driving_mins\",new_column_3)\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_4=F.when(df1.peak_end_time > df1.peak_start_time,(to_timestamp(df1.peak_end_time,normaltimeFormat).cast(\"long\")\n",
    "                                   -to_timestamp(df1.peak_start_time,normaltimeFormat).cast(\"long\"))/60)\\\n",
    ".otherwise(0)\n",
    "df1=df1.withColumn(\"peak_driving_mins\",new_column_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|    peak_start_time|      peak_end_time|      driving_mins|peak_driving_mins|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 07:00:00|2018-06-29 09:00:00|2018-06-29 07:00:00|2018-06-29 06:47:22| 9.516666666666667|              0.0|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 07:00:00|2018-07-07 09:00:00|2018-07-07 07:00:00|2018-07-07 01:19:43|19.983333333333334|              0.0|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(to_date(df1.start_day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df=df1.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(peak_driving_mins)\").alias(\"daily_peak_driving_mins\"),\n",
    "                                  expr(\"sum(driving_mins)\").alias(\"daily_driving_mins\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df=agg_df.withColumn(\"percent_daily_peak_driving\", (F.col(\"daily_peak_driving_mins\") / F.col(\"daily_driving_mins\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-----------------------+------------------+--------------------------+\n",
      "|              vin|start_day|daily_peak_driving_mins|daily_driving_mins|percent_daily_peak_driving|\n",
      "+-----------------+---------+-----------------------+------------------+--------------------------+\n",
      "|LMGMS1G83J1005132| 20180425|                  61.35|243.73333333333335|       0.25170951859956237|\n",
      "|LMGMS1G85H1S00272| 20180712|     29.316666666666666|417.21666666666664|       0.07026724723365159|\n",
      "+-----------------+---------+-----------------------+------------------+--------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"select vin, start_day, min(start_time) as start_time from ubi.conv_trips_complete group by vin, start_day\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = hive2spark(hc, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------------------+\n",
      "|              vin|start_day|         start_time|\n",
      "+-----------------+---------+-------------------+\n",
      "|LMGMS1G80H1000013| 20180121|2018-01-21 00:21:10|\n",
      "|LMGMS1G80H1000013| 20181224|2018-12-24 04:49:33|\n",
      "+-----------------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1=df1.withColumn(\"thirty_day_after\", F.date_add(df1['start_day'].cast(\"date\"), 30))\n",
    "#months_to_add = 1\n",
    "#df1 = df1.withColumn(\"thirty_day_after\", F.add_months(df1.start_day.cast(\"date\"), months_to_add))\n",
    "#from datetime import timedelta\n",
    "#df1.withColumn(\"thirty_day_after\", expr(\"date_add(\"start_day\",30)\")\n",
    "#df1=df1.withColumn(\"thirty_day_after\", date_add(to_date(col('start_day').cast('string')),30))\n",
    "#df1=df1.withColumn(\"thirty_day_after\", datetime.strptime(df1['start_day'].cast(\"string\"),'%Y%m%d'))\n",
    "#df1=df1.withColumn(\"thirty_day_after\", df1['start_day'].cast('date'))\n",
    "agg_df=agg_df.withColumn(\"thirty_day_after\", date_add(to_timestamp(df1.start_day,\"yyyyMMdd\"),30))\n",
    "#(to_timestamp(df1.end_time.astype('Timestamp'),normaltimeFormat).cast(\"long\")\n",
    " #               -to_timestamp(df1.start_time.astype('Timestamp'),normaltimeFormat).cast(\"long\"))/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-----------------------+------------------+--------------------------+----------------+\n",
      "|              vin|start_day|daily_peak_driving_mins|daily_driving_mins|percent_daily_peak_driving|thirty_day_after|\n",
      "+-----------------+---------+-----------------------+------------------+--------------------------+----------------+\n",
      "|LMGMS1G81J1005498| 20180630|     12.533333333333333| 52.88333333333333|       0.23699968484084463|      2018-07-30|\n",
      "|LMGMS1G89J1014093| 20190309|                    0.0|140.28333333333336|                       0.0|      2019-04-08|\n",
      "+-----------------+---------+-----------------------+------------------+--------------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df.show(2)\n",
    "#df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-----------------------+------------------+\n",
      "|              vin|start_day|daily_peak_driving_mins|daily_driving_mins|\n",
      "+-----------------+---------+-----------------------+------------------+\n",
      "|LMGMS1G80J1001278| 20181207|                    0.0| 4.283333333333333|\n",
      "|LMGMS1G80J1001278| 20181207|     52.516666666666666|110.98333333333332|\n",
      "+-----------------+---------+-----------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df2.alias(\"a\").join(agg_df.alias(\"b\"), (agg_df.vin == df2.vin) & (df2.start_day <= agg_df.thirty_day_after) & (df2.start_day > agg_df.start_day))\\\n",
    ".select(\"a.vin\",\"a.start_day\",\"b.daily_peak_driving_mins\",\"b.daily_driving_mins\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df=df.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(daily_peak_driving_mins)\").alias(\"daily_peak_driving_mins\"),\n",
    "                                  expr(\"sum(daily_driving_mins)\").alias(\"daily_driving_mins\"))\n",
    "res_df=res_df.withColumn(\"percent_daily_peak_driving\", (F.col(\"daily_peak_driving_mins\") / F.col(\"daily_driving_mins\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE ubi.temp_am4 AS SELECT vin,start_day, daily_peak_driving_mins, daily_driving_mins,\n",
      "            percent_daily_peak_driving FROM update_dataframe\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'`ubi`.`temp_am4` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-841496f5af17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             percent_daily_peak_driving FROM update_dataframe\"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Table temp_am4 creation done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'`ubi`.`temp_am4` already exists.;'"
     ]
    }
   ],
   "source": [
    "cols = [when(~col(x).isin(\"NULL\", \"NA\", \"NaN\",\"\"), col(x)).alias(x) for x in res_df.columns]\n",
    "res_df = res_df.select(*cols)\n",
    "res_df.registerTempTable('update_dataframe')\n",
    "sql_cmd = \"\"\"CREATE TABLE ubi.temp_am4 AS SELECT vin,start_day, daily_peak_driving_mins, daily_driving_mins,\n",
    "            percent_daily_peak_driving FROM update_dataframe\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd)\n",
    "print('Table temp_am4 creation done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select vin, distance, start_day, case\n",
    "  when distance >= 100 then 1\n",
    "  else 0\n",
    "  end as long_distance\n",
    " from ubi.conv_trips_complete\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = hive2spark(hc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vin=u'LMGMS1G80H1000254', distance=3.0, start_day=u'20180629', long_distance=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------+-------------+\n",
      "|              vin|distance|start_day|long_distance|\n",
      "+-----------------+--------+---------+-------------+\n",
      "|LMGMS1G80H1000254|     3.0| 20180629|            0|\n",
      "|LMGMS1G80J1007761|     8.0| 20180707|            0|\n",
      "+-----------------+--------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_b = df_a.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(long_distance)\").alias(\"frequency\"),expr(\"avg(long_distance)\").alias(\"percent_long_distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vin=u'LMGMS1G83J1006264', start_day=u'20180624', frequency=0, percent_long_distance=0.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+---------------------+\n",
      "|              vin|start_day|frequency|percent_long_distance|\n",
      "+-----------------+---------+---------+---------------------+\n",
      "|LMGMS1G82J1006787| 20180914|        0|                  0.0|\n",
      "|LMGMS1G84J1015121| 20181018|        0|                  0.0|\n",
      "|LMGMS1G84J1020691| 20190407|        0|                  0.0|\n",
      "|LMGMS1G83J1010864| 20180819|        1| 0.030303030303030304|\n",
      "|LMGMS1G8XJ1013020| 20190409|        1| 0.047619047619047616|\n",
      "+-----------------+---------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df_b.withColumn(\"thirty_day_after\", date_add(to_timestamp(df_b.start_day,\"yyyyMMdd\"),30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+---------------------+----------------+\n",
      "|              vin|start_day|frequency|percent_long_distance|thirty_day_after|\n",
      "+-----------------+---------+---------+---------------------+----------------+\n",
      "|LMGMS1G80J1018808| 20181225|        0|                  0.0|      2019-01-24|\n",
      "|LMGMS1G82J1001752| 20181111|        0|                  0.0|      2018-12-11|\n",
      "|LMGMS1G87J1006218| 20190301|        0|                  0.0|      2019-03-31|\n",
      "|LMGMS1G82J1022360| 20190206|        0|                  0.0|      2019-03-08|\n",
      "|LMGMS1G83J1000982| 20180728|        1|                 0.25|      2018-08-27|\n",
      "+-----------------+---------+---------+---------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+---------------------+\n",
      "|              vin|start_day|frequency|percent_long_distance|\n",
      "+-----------------+---------+---------+---------------------+\n",
      "|LMGMS1G80J1001278| 20181225|        0|                  0.0|\n",
      "|LMGMS1G80J1001278| 20181225|        0|                  0.0|\n",
      "|LMGMS1G80J1001278| 20181225|        0|                  0.0|\n",
      "|LMGMS1G80J1001278| 20181225|        0|                  0.0|\n",
      "|LMGMS1G80J1001278| 20181225|        0|                  0.0|\n",
      "+-----------------+---------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_d = df2.alias(\"c\").join(df_b.alias(\"b\"), (df_b.vin == df2.vin) & (df2.start_day <= df_b.thirty_day_after) & (df2.start_day > df_b.start_day))\\\n",
    ".select(\"c.vin\",\"c.start_day\",\"b.frequency\",\"b.percent_long_distance\") \n",
    "df_d.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+---------------------+\n",
      "|              vin|start_day|frequency|percent_long_distance|\n",
      "+-----------------+---------+---------+---------------------+\n",
      "|LMGMS1G80J1001278| 20181207|        0|                  0.0|\n",
      "|LMGMS1G80J1001278| 20181225|        3|                 0.05|\n",
      "|LMGMS1G80J1001278| 20181230|        3| 0.039473684210526314|\n",
      "|LMGMS1G80J1001278| 20181220|        3|  0.06818181818181818|\n",
      "|LMGMS1G80J1001278| 20181229|        3| 0.041666666666666664|\n",
      "+-----------------+---------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ld = df_d.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(frequency)\").alias(\"frequency\"),expr(\"avg(percent_long_distance)\").alias(\"percent_long_distance\"))\n",
    "df_ld.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE ubi.temp_ld AS SELECT vin,start_day, frequency, percent_long_distance FROM long_distance\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'`ubi`.`temp_ld` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a21e951fadc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msql_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"CREATE TABLE ubi.temp_ld AS SELECT vin,start_day, frequency, percent_long_distance FROM long_distance\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Table temp_ld creation done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'`ubi`.`temp_ld` already exists.;'"
     ]
    }
   ],
   "source": [
    "cols = [when(~col(x).isin(\"NULL\", \"NA\", \"NaN\",\"\"), col(x)).alias(x) for x in df_ld.columns]\n",
    "df_ld = df_ld.select(*cols)\n",
    "df_ld.registerTempTable('long_distance')\n",
    "sql_cmd = \"\"\"CREATE TABLE ubi.temp_ld AS SELECT vin,start_day, frequency, percent_long_distance FROM long_distance\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd)\n",
    "print('Table temp_ld creation done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Night Driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select vin, start_time, start_day, end_time, \n",
    "     from_unixtime(unix_timestamp(CONCAT(start_day, ' ', '22:00:00'), 'yyyyMMdd HH:mm:ss')) as night_start,\n",
    "   from_unixtime(unix_timestamp(CONCAT(start_day, ' ', '05:00:00'), 'yyyyMMdd HH:mm:ss') + 86400) as night_end from ubi.conv_trips_complete\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = hive2spark(hc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vin: string, start_time: string, start_day: string, end_time: timestamp, night_start: string, night_end: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.withColumn(\"start_time\",df_a.start_time.astype('Timestamp'))\n",
    "df_a.withColumn(\"end_time\",df_a.end_time.astype('Timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "|              vin|         start_time|start_day|           end_time|        night_start|          night_end|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 22:00:00|2018-06-30 05:00:00|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 22:00:00|2018-07-08 05:00:00|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_a.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_1=F.when(df_a.start_time > df_a.night_start,df_a.start_time.astype('Timestamp'))\\\n",
    ".otherwise(df_a.night_start.astype('Timestamp'))\n",
    "new_column_2=F.when(df_a.end_time < df_a.night_end,df_a.end_time.astype('Timestamp'))\\\n",
    ".otherwise(df_a.night_end.astype('Timestamp'))\n",
    "df_a = df_a.withColumn(\"night_start_time\",new_column_1)\n",
    "df_b = df_a.withColumn(\"night_end_time\",new_column_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|              vin|         start_time|start_day|           end_time|        night_start|          night_end|   night_start_time|     night_end_time|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 22:00:00|2018-06-30 05:00:00|2018-06-29 22:00:00|2018-06-29 06:47:22|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 22:00:00|2018-07-08 05:00:00|2018-07-07 22:00:00|2018-07-07 01:19:43|\n",
      "|LMGMS1G80J1009719|2018-08-14 02:18:32| 20180814|2018-08-14 02:24:00|2018-08-14 22:00:00|2018-08-15 05:00:00|2018-08-14 22:00:00|2018-08-14 02:24:00|\n",
      "|LMGMS1G80J1015701|2019-01-12 01:15:52| 20190112|2019-01-12 01:39:32|2019-01-12 22:00:00|2019-01-13 05:00:00|2019-01-12 22:00:00|2019-01-12 01:39:32|\n",
      "|LMGMS1G80J1017061|2019-02-05 01:53:03| 20190205|2019-02-05 02:13:11|2019-02-05 22:00:00|2019-02-06 05:00:00|2019-02-05 22:00:00|2019-02-05 02:13:11|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_3 = (to_timestamp(df_b.end_time.astype('Timestamp'),normaltimeFormat).cast(\"long\")\n",
    "                -to_timestamp(df_b.start_time.astype('Timestamp'),normaltimeFormat).cast(\"long\"))/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|              vin|         start_time|start_day|           end_time|        night_start|          night_end|   night_start_time|     night_end_time|      driving_mins|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 22:00:00|2018-06-30 05:00:00|2018-06-29 22:00:00|2018-06-29 06:47:22| 9.516666666666667|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 22:00:00|2018-07-08 05:00:00|2018-07-07 22:00:00|2018-07-07 01:19:43|19.983333333333334|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_c = df_b.withColumn(\"driving_mins\",new_column_3)\n",
    "df_c.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|              vin|         start_time|start_day|           end_time|        night_start|          night_end|   night_start_time|     night_end_time|      driving_mins|night_driving_mins|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 22:00:00|2018-06-30 05:00:00|2018-06-29 22:00:00|2018-06-29 06:47:22| 9.516666666666667|               0.0|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 22:00:00|2018-07-08 05:00:00|2018-07-07 22:00:00|2018-07-07 01:19:43|19.983333333333334|               0.0|\n",
      "|LMGMS1G80J1009719|2018-08-14 02:18:32| 20180814|2018-08-14 02:24:00|2018-08-14 22:00:00|2018-08-15 05:00:00|2018-08-14 22:00:00|2018-08-14 02:24:00| 5.466666666666667|               0.0|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_column_4=F.when(df_c.night_end_time > df_c.night_start_time,(to_timestamp(df_c.night_end_time,normaltimeFormat).cast(\"long\")\n",
    "                                   -to_timestamp(df_c.night_start_time,normaltimeFormat).cast(\"long\"))/60)\\\n",
    ".otherwise(0)\n",
    "df_c=df_c.withColumn(\"night_driving_mins\",new_column_4)\n",
    "df_c.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_c.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(night_driving_mins)\").alias(\"daily_night_driving_mins\"),\n",
    "                                           expr(\"sum(driving_mins)\").alias(\"daily_driving_mins\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------------------+------------------+\n",
      "|              vin|start_day|daily_night_driving_mins|daily_driving_mins|\n",
      "+-----------------+---------+------------------------+------------------+\n",
      "|LMGMS1G86J1011104| 20180821|                     0.0|301.80000000000007|\n",
      "|LMGMS1G84J1013255| 20180706|                     0.0|             263.7|\n",
      "+-----------------+---------+------------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_d.withColumn(\"thirty_day_after\", date_add(to_timestamp(df_c.start_day,\"yyyyMMdd\"),30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------------------+------------------+----------------+\n",
      "|              vin|start_day|daily_night_driving_mins|daily_driving_mins|thirty_day_after|\n",
      "+-----------------+---------+------------------------+------------------+----------------+\n",
      "|LMGMS1G81J1001869| 20190223|                     0.0| 93.11666666666667|      2019-03-25|\n",
      "|LMGMS1G84J1019234| 20181229|                     0.0|             144.3|      2019-01-28|\n",
      "+-----------------+---------+------------------------+------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------------+------------------------+\n",
      "|              vin|start_day|daily_driving_mins|daily_night_driving_mins|\n",
      "+-----------------+---------+------------------+------------------------+\n",
      "|LMGMS1G80J1001278| 20181209|             209.8|                     0.0|\n",
      "|LMGMS1G80J1001278| 20181209|110.98333333333332|                     0.0|\n",
      "|LMGMS1G80J1001278| 20181209| 4.283333333333333|                     0.0|\n",
      "|LMGMS1G80J1001278| 20181209|237.76666666666668|                     0.0|\n",
      "|LMGMS1G80J1001278| 20181221|             209.8|                     0.0|\n",
      "+-----------------+---------+------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f = df2.alias(\"e\").join(df_d.alias(\"d\"), (df_d.vin == df2.vin) & (df2.start_day <= df_d.thirty_day_after) & (df2.start_day > df_d.start_day))\\\n",
    ".select(\"e.vin\",\"e.start_day\",\"d.daily_driving_mins\",\"d.daily_night_driving_mins\") \n",
    "df_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------------------+------------------+---------------------------+\n",
      "|              vin|start_day|daily_night_driving_mins|daily_driving_mins|percent_daily_night_driving|\n",
      "+-----------------+---------+------------------------+------------------+---------------------------+\n",
      "|LMGMS1G80J1001278| 20181206|                     0.0|110.98333333333332|                        0.0|\n",
      "|LMGMS1G80J1001278| 20181220|      125.96666666666667|1467.8166666666668|        0.08581907368086386|\n",
      "+-----------------+---------+------------------------+------------------+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nd=df_f.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(daily_night_driving_mins)\").alias(\"daily_night_driving_mins\"),\n",
    "                                  expr(\"sum(daily_driving_mins)\").alias(\"daily_driving_mins\"))\n",
    "df_nd=df_nd.withColumn(\"percent_daily_night_driving\", (F.col(\"daily_night_driving_mins\") / F.col(\"daily_driving_mins\")))\n",
    "df_nd.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE ubi.temp_nd AS SELECT vin,start_day, daily_night_driving_mins, daily_driving_mins,  \n",
      "            percent_daily_night_driving FROM night_driving\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'`ubi`.`temp_nd` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5111c3eb4214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             percent_daily_night_driving FROM night_driving\"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Table temp_nd creation done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'`ubi`.`temp_nd` already exists.;'"
     ]
    }
   ],
   "source": [
    "cols = [when(~col(x).isin(\"NULL\", \"NA\", \"NaN\",\"\"), col(x)).alias(x) for x in df_nd.columns]\n",
    "df_nd = df_nd.select(*cols)\n",
    "df_nd.registerTempTable('night_driving')\n",
    "sql_cmd = \"\"\"CREATE TABLE ubi.temp_nd AS SELECT vin,start_day, daily_night_driving_mins, daily_driving_mins,  \n",
    "            percent_daily_night_driving FROM night_driving\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd)\n",
    "print('Table temp_nd creation done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM Peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select vin, start_time, start_day, end_time, \n",
    "   from_unixtime(unix_timestamp(CONCAT(start_day, ' ', '16:00:00'), 'yyyyMMdd HH:mm:ss')) as peak_start,\n",
    "   from_unixtime(unix_timestamp(CONCAT(start_day, ' ', '19:00:00'), 'yyyyMMdd HH:mm:ss')) as peak_end \n",
    "   from ubi.conv_trips_complete\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = hive2spark(hc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_1=F.when(df_a.start_time > df_a.peak_start,df_a.start_time.astype('Timestamp'))\\\n",
    ".otherwise(df_a.peak_start.astype('Timestamp'))\n",
    "new_column_2=F.when(df_a.end_time < df_a.peak_end,df_a.end_time.astype('Timestamp'))\\\n",
    ".otherwise(df_a.peak_end.astype('Timestamp'))\n",
    "df_a=df_a.withColumn(\"peak_start_time\",new_column_1)\n",
    "df_b=df_a.withColumn(\"peak_end_time\",new_column_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|    peak_start_time|      peak_end_time|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 16:00:00|2018-06-29 19:00:00|2018-06-29 16:00:00|2018-06-29 06:47:22|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 16:00:00|2018-07-07 19:00:00|2018-07-07 16:00:00|2018-07-07 01:19:43|\n",
      "|LMGMS1G80J1009719|2018-08-14 02:18:32| 20180814|2018-08-14 02:24:00|2018-08-14 16:00:00|2018-08-14 19:00:00|2018-08-14 16:00:00|2018-08-14 02:24:00|\n",
      "|LMGMS1G80J1015701|2019-01-12 01:15:52| 20190112|2019-01-12 01:39:32|2019-01-12 16:00:00|2019-01-12 19:00:00|2019-01-12 16:00:00|2019-01-12 01:39:32|\n",
      "|LMGMS1G80J1017061|2019-02-05 01:53:03| 20190205|2019-02-05 02:13:11|2019-02-05 16:00:00|2019-02-05 19:00:00|2019-02-05 16:00:00|2019-02-05 02:13:11|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|              vin|         start_time|start_day|           end_time|         peak_start|           peak_end|    peak_start_time|      peak_end_time|      driving_mins|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "|LMGMS1G80H1000254|2018-06-29 06:37:51| 20180629|2018-06-29 06:47:22|2018-06-29 16:00:00|2018-06-29 19:00:00|2018-06-29 16:00:00|2018-06-29 06:47:22| 9.516666666666667|\n",
      "|LMGMS1G80J1007761|2018-07-07 00:59:44| 20180707|2018-07-07 01:19:43|2018-07-07 16:00:00|2018-07-07 19:00:00|2018-07-07 16:00:00|2018-07-07 01:19:43|19.983333333333334|\n",
      "+-----------------+-------------------+---------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_column_3 = (to_timestamp(df_b.end_time.astype('Timestamp'),normaltimeFormat).cast(\"long\")\n",
    "                -to_timestamp(df_b.start_time.astype('Timestamp'),normaltimeFormat).cast(\"long\"))/60\n",
    "df_b = df_b.withColumn(\"driving_mins\",new_column_3)\n",
    "df_b.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_4=F.when(df_b.peak_end_time > df_b.peak_start_time,(to_timestamp(df_b.peak_end_time,normaltimeFormat).cast(\"long\")\n",
    "                                   -to_timestamp(df_b.peak_start_time,normaltimeFormat).cast(\"long\"))/60)\\\n",
    ".otherwise(0)\n",
    "df_c=df_b.withColumn(\"peak_driving_mins\",new_column_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=df_c.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(peak_driving_mins)\").alias(\"daily_peak_driving_mins\"),\n",
    "                                  expr(\"sum(driving_mins)\").alias(\"daily_driving_mins\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d=df_c.withColumn(\"thirty_day_after\", date_add(to_timestamp(df_c.start_day,\"yyyyMMdd\"),30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-----------------------+------------------+----------------+\n",
      "|              vin|start_day|daily_peak_driving_mins|daily_driving_mins|thirty_day_after|\n",
      "+-----------------+---------+-----------------------+------------------+----------------+\n",
      "|LMGMS1G82J1006787| 20180914|                    0.0|168.41666666666666|      2018-10-14|\n",
      "|LMGMS1G84J1015121| 20181018|                    0.0|             383.2|      2018-11-17|\n",
      "+-----------------+---------+-----------------------+------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------------------+-----------------------+\n",
      "|              vin|start_day|daily_driving_mins|daily_peak_driving_mins|\n",
      "+-----------------+---------+------------------+-----------------------+\n",
      "|LMGMS1G80J1001278| 20181212| 4.283333333333333|                    0.0|\n",
      "|LMGMS1G80J1001278| 20181212| 95.60000000000001|                    0.0|\n",
      "|LMGMS1G80J1001278| 20181212|             209.8|                    0.0|\n",
      "|LMGMS1G80J1001278| 20181212| 82.93333333333334|                    0.0|\n",
      "|LMGMS1G80J1001278| 20181212|237.76666666666668|                    0.0|\n",
      "+-----------------+---------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f = df2.alias(\"e\").join(df_d.alias(\"d\"), (df_d.vin == df2.vin) & (df2.start_day <= df_d.thirty_day_after) & (df2.start_day > df_d.start_day))\\\n",
    ".select(\"e.vin\",\"e.start_day\",\"d.daily_driving_mins\",\"d.daily_peak_driving_mins\") \n",
    "df_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vin: string, start_day: string, daily_peak_driving_mins: double, daily_driving_mins: double, percent_daily_peak_driving: double]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pm=df_f.groupBy(\"vin\",\"start_day\").agg(expr(\"sum(daily_peak_driving_mins)\").alias(\"daily_peak_driving_mins\"),\n",
    "                                  expr(\"sum(daily_driving_mins)\").alias(\"daily_driving_mins\"))\n",
    "df_pm.withColumn(\"percent_daily_peak_driving\", (F.col(\"daily_peak_driving_mins\") / F.col(\"daily_driving_mins\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE ubi.temp_pm AS SELECT vin,start_day, daily_peak_driving_mins, daily_driving_mins,  \n",
      "            percent_daily_peak_driving FROM pm_peak\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u\"Invalid call to dataType on unresolved object, tree: 'percent_daily_peak_driving\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-5aaccbd9273e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             percent_daily_peak_driving FROM pm_peak\"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Table temp_pm creation done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Invalid call to dataType on unresolved object, tree: 'percent_daily_peak_driving\""
     ]
    }
   ],
   "source": [
    "cols = [when(~col(x).isin(\"NULL\", \"NA\", \"NaN\",\"\"), col(x)).alias(x) for x in df_pm.columns]\n",
    "df_pm = df_pm.select(*cols)\n",
    "df_pm.registerTempTable('pm_peak')\n",
    "sql_cmd = \"\"\"CREATE TABLE ubi.temp_pm AS SELECT vin,start_day, daily_peak_driving_mins, daily_driving_mins,  \n",
    "            percent_daily_peak_driving FROM pm_peak\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd)\n",
    "print('Table temp_pm creation done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select vin, count(*) as totalfrequency from ubi.conv_trips_complete group by vin\"\"\" \n",
    "df_f = hive2spark(hc, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip = hive2spark(hc, \"select * from ubi.conv_trips_complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+------------------+------------------+--------------+\n",
      "|              vin|     start_loc_lat|     start_loc_lon|       end_loc_lat|       end_loc_lon|totalfrequency|\n",
      "+-----------------+------------------+------------------+------------------+------------------+--------------+\n",
      "|LMGMS1G80J1001278|22.973869444444443|114.70871944444444|22.975155555555553|114.70485833333333|           997|\n",
      "|LMGMS1G80J1001278|22.977430555555554|114.71240833333334|22.975330555555555|114.70467222222223|           997|\n",
      "|LMGMS1G80J1001278|22.974494444444442|114.70517222222223|22.969052777777776|114.70706666666666|           997|\n",
      "|LMGMS1G80J1001278|22.973863888888886|114.70868333333334|22.974852777777777|          114.7047|           997|\n",
      "|LMGMS1G80J1001278| 22.97539722222222|114.70496666666666|23.086427777777775|114.47661111111111|           997|\n",
      "+-----------------+------------------+------------------+------------------+------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f = df_trip.alias(\"e\").join(df_f.alias(\"f\"), df_trip.vin == df_f.vin)\\\n",
    ".select(\"e.vin\",\"e.start_loc_lat\",\"e.start_loc_lon\",\"e.end_loc_lat\",\"end_loc_lon\",\"f.totalfrequency\") \n",
    "df_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------------------+------------------+------------------+--------------+\n",
      "|              vin|start_loc_lat|     start_loc_lon|       end_loc_lat|       end_loc_lon|totalfrequency|\n",
      "+-----------------+-------------+------------------+------------------+------------------+--------------+\n",
      "|LMGMS1G80J1001278|       22.468|112.75563888888888|22.363736111111113|112.68298611111112|           997|\n",
      "|LMGMS1G80J1001278|       22.591|114.14512500000001|22.974899999999998| 114.7047388888889|           997|\n",
      "|LMGMS1G80J1001278|       22.993|114.71179166666667|22.974922222222222|114.70473611111112|           997|\n",
      "|LMGMS1G80J1001278|       22.601|114.04439444444444|22.567680555555555|114.05658888888888|           997|\n",
      "+-----------------+-------------+------------------+------------------+------------------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df4 = df_f.select(F.round(F.col(\"start_lon_lat\").cast(\"float\"),3).alias(\"start_loc_lat\"))\n",
    "df_f = df_f.withColumn(\"start_loc_lat\", round(\"start_loc_lat\", 3))\n",
    "df_f.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f.withColumn(\"start_loc_lon\", round(\"start_loc_lat\", 3))\n",
    "df_f = df_f.withColumn(\"end_loc_lat\", round(\"end_loc_lat\", 3))\n",
    "df_f = df_f.withColumn(\"end_loc_lon\", round(\"end_loc_lon\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+-----------+-----------+--------------+\n",
      "|              vin|start_loc_lat|start_loc_lon|end_loc_lat|end_loc_lon|totalfrequency|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------+\n",
      "|LMGMS1G80J1001278|       22.977|       22.977|     22.975|    114.705|           997|\n",
      "|LMGMS1G80J1001278|       22.974|       22.974|     22.969|    114.707|           997|\n",
      "|LMGMS1G80J1001278|       22.974|       22.974|     22.975|    114.705|           997|\n",
      "|LMGMS1G80J1001278|        22.98|        22.98|     22.975|    114.705|           997|\n",
      "|LMGMS1G80J1001278|       22.578|       22.578|     22.557|    114.128|           997|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f.withColumn(\"probability\",  (1 / F.col(\"totalfrequency\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+-----------+-----------+--------------+--------------------+\n",
      "|              vin|start_loc_lat|start_loc_lon|end_loc_lat|end_loc_lon|totalfrequency|         probability|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------+--------------------+\n",
      "|LMGMS1G80J1001278|       22.578|       22.578|     22.568|    114.056|           997|0.001003009027081...|\n",
      "|LMGMS1G80J1001278|        22.98|        22.98|     22.975|    114.705|           997|0.001003009027081...|\n",
      "|LMGMS1G80J1001278|       22.969|       22.969|     22.972|    114.711|           997|0.001003009027081...|\n",
      "|LMGMS1G80J1001278|       23.085|       23.085|     23.087|    114.479|           997|0.001003009027081...|\n",
      "|LMGMS1G80J1001278|       22.954|       22.954|     22.975|    114.705|           997|0.001003009027081...|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+-----------+-----------+--------------------+\n",
      "|              vin|start_loc_lat|start_loc_lon|end_loc_lat|end_loc_lon|         probability|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------------+\n",
      "|LMGMS1G80J1001278|       22.578|       22.578|     22.568|    114.056|0.001003009027081...|\n",
      "|LMGMS1G80J1001278|       22.974|       22.974|     22.975|    114.705|0.001003009027081...|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_f = df_f.drop(F.col(\"totalfrequency\"))\n",
    "df_f.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+-----------+-----------+--------------------+\n",
      "|              vin|start_loc_lat|start_loc_lon|end_loc_lat|end_loc_lon|         probability|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------------+\n",
      "|LMGMS1G80J1001278|       22.468|       22.468|     22.364|    112.683|0.001003009027081...|\n",
      "|LMGMS1G80J1001278|       22.591|       22.591|     22.975|    114.705|0.003009027081243...|\n",
      "|LMGMS1G80J1001278|       22.975|       22.975|     23.086|    114.477|0.002006018054162...|\n",
      "|LMGMS1G80J1001278|       22.988|       22.988|     22.975|    114.705|0.007021063189568...|\n",
      "|LMGMS1G80J1001278|       22.601|       22.601|     22.568|    114.057|0.001003009027081...|\n",
      "+-----------------+-------------+-------------+-----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_g = df_f.groupBy(\"vin\",\"start_loc_lat\",\"start_loc_lon\", \"end_loc_lat\", \"end_loc_lon\")\\\n",
    "            .agg(expr(\"sum(probability)\").alias(\"probability\"))\n",
    "df_g.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|              vin|      entropy_all|\n",
      "+-----------------+-----------------+\n",
      "|LMGMS1G80J1001278| 8.83199050068945|\n",
      "|LMGMS1G80J1004780|7.954114910693735|\n",
      "|LMGMS1G80J1014239|6.336553572571366|\n",
      "|LMGMS1G80J1014600| 8.73289802020143|\n",
      "|LMGMS1G80J1014726|9.339963602427142|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_h = df_g.groupBy(\"vin\").agg(expr(\"sum(-probability * log2(probability))\").alias(\"entropy_all\"))\n",
    "df_h.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
