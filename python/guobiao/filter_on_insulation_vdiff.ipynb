{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# spark location on namenode server\n",
    "findspark.init(\"/usr/hdp/current/spark2-client\")\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAll([('spark.app.name', 'guobiao_tsp_tbls.guobiao_vehilce_filter'), # App Name\n",
    "    ('spark.master', 'yarn'),              # spark run mode: locally or remotely\n",
    "    ('spark.submit.deployMode', 'client'), # deploy in yarn-client or yarn-cluster\n",
    "    ('spark.executor.memory', '8g'),       # memory allocated for each executor\n",
    "    ('spark.executor.cores', '3'),         # number of cores for each executor\n",
    "    ('spark.executor.instances', '1'),    # number of executors in total\n",
    "    ('spark.driver.maxResultSize', '5g'), # Result size is large, need to increase from default of 1g\n",
    "    ('spark.yarn.am.memory', '10g')])       # memory for spark driver (application master)\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "d = datetime.now().date()\n",
    "delta = timedelta(days=2)\n",
    "days=[]\n",
    "for i in range(delta.days+1):\n",
    "    days.append(d-timedelta(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport sys\\n   \\n# Import path to zipfile\\nZIP_FILE = '/home/wchen/guobiao/veh.zip'\\nsys.path.insert(0, ZIP_FILE)\\nfrom trip_based_stats.configs import *\\nimport trip_based_stats.single_vin_battery_flow as bf\\nfrom common.vehicle_utils import load_header_config\\n\\nconfig_file = '/home/wchen/guobiao/guobiao_header.csv'\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sys\n",
    "   \n",
    "# Import path to zipfile\n",
    "ZIP_FILE = '/home/wchen/guobiao/veh.zip'\n",
    "sys.path.insert(0, ZIP_FILE)\n",
    "from trip_based_stats.configs import *\n",
    "import trip_based_stats.single_vin_battery_flow as bf\n",
    "from common.vehicle_utils import load_header_config\n",
    "\n",
    "config_file = '/home/wchen/guobiao/guobiao_header.csv'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_cmd(args_list):\n",
    "    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    proc.communicate()\n",
    "    return proc.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseline(line):\n",
    "    fields=line.split(',')\n",
    "    vin=str(fields[0])    #no conversion would be string\n",
    "    #other_fields=[]\n",
    "    vintype=str(fields[1])\n",
    "    veh_insulation=str(fields[13])\n",
    "    esd_sc_volt_list=str(fields[77]).split('|')  \n",
    "    #esd_sc_volt_list=list(map(float, str(fields[77]).split('|')))        \n",
    "    #other_fields.append(vintype)\n",
    "    #other_fields.append(veh_insulation)\n",
    "    #other_fields.append(esd_sc_volt_list)\n",
    "    return (vin, vintype, [esd_sc_volt_list, [veh_insulation]])\n",
    "    #return (vin, vintype, veh_insulation, esd_sc_volt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "spark=SparkSession.builder.config(\"spark.sql.warehouse.dir\").appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "data_files=[]\n",
    "for day in days:\n",
    "    day = day.isoformat()\n",
    "    dd = day.replace('-', '')\n",
    "    df_name='hdfs://172.15.7.170:8020/data/guobiao/csv/d={}'.format(dd)\n",
    "    returncode = run_cmd(['hdfs', 'dfs', '-test', '-e', df_name])\n",
    "    if returncode:\n",
    "        print('{} does not exist, skipping ..'.format(df_name))\n",
    "        continue    \n",
    "    data_files.append(df_name)\n",
    "\n",
    "\n",
    "\n",
    "if len(data_files) == 1:\n",
    "    rdd = spark.sparkContext.textFile(data_files[0])\n",
    "else:\n",
    "    rdds = []\n",
    "    for data_file in data_files:\n",
    "\n",
    "        rddi = spark.sparkContext.textFile(data_file)\n",
    "        rdds.append(rddi)\n",
    "        rdd = sc.union(rdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge3Lines=rdd.map(parseline).filter(lambda x: \"A5HEV\" in x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+--------------------+\n",
      "|               _1|   _2|                  _3|\n",
      "+-----------------+-----+--------------------+\n",
      "|LMGHP1S57H1000607|A5HEV|[WrappedArray(3.6...|\n",
      "|LMGHP1S54J1003938|A5HEV|[WrappedArray(3.9...|\n",
      "|LMGHP1S54J1002806|A5HEV|[WrappedArray(3.9...|\n",
      "|LMGHP1S83J1001897|A5HEV|[WrappedArray(4.0...|\n",
      "|LMGHP1S5XJ1003751|A5HEV|[WrappedArray(3.6...|\n",
      "+-----------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ge3df=ge3Lines.toDF()\n",
    "ge3df.createOrReplaceTempView(\"ge3_df\")\n",
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM ge3_df\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ge3Lines.toDF().take(3)\n",
    "midresult = ge3Lines.map(lambda x: (x[0], x[2])).reduceByKey(lambda x, y: [list(x[0]) + list(y[0]), list(x[1]) + list(y[1])])\n",
    "#midresult = ge3Lines.map(lambda x: (x[0], x[2])).reduceByKey(lambda x, y: (list(x) + list(y)))\n",
    "\n",
    "    #.map(lambda x: (x[0], max(float(x[1])), min(float(x[1])), sum(float(x[1])), len(x[1])))\n",
    "    #.reduceByKey(lambda x, y: (max(x[0], y[0]), min(x[1], y[1]), sum(x[2], y[2]), sum(x[3], y[3]))).collect()\n",
    "#    \n",
    "#volt = midresult.reduceByKey(lambda x, y: (min(x,y),max(x,y),(x+y)))\n",
    "#df=volt.toDF(\"vin\",\"min\",\"max\",\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "midresult = midresult.map(lambda x: (x[0], [float(i) for i in x[1][0] if i and i.replace('.', '', 1).isdigit()], [float(i) for i in x[1][1] if i and i.replace('.', '', 1).isdigit()]))\\\n",
    "            .filter(lambda x: x[1] and x[2])\\\n",
    "            .map(lambda x: (x[0], max(x[1]), min(x[1]), sum(x[1]), len(x[1]), max(x[2]), min(x[2]), sum(x[2]), len(x[2])))\\\n",
    "            .map(lambda x: (x[0], x[1], x[2], x[3] / x[4], x[5], x[6], x[7] / x[8]))\n",
    "#midresult = midresult.map(lambda x: (x[0], [float(i) for i in x[1] if i and i.replace('.', '', 1).isdigit()]))\\\n",
    "#            .map(lambda x: (x[0], max(x[1]), min(x[1]), sum(x[1]), len(x[1])))\\\n",
    "#            .map(lambda x: (x[0], x[1], x[2], x[3] / x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDf=midresult.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'LMGHP1S57J1003111', _2=4.2, _3=3.497, _4=3.8754369751912128, _5=1000.0, _6=1000.0, _7=1000.0),\n",
       " Row(_1=u'LMGHP1S52J1004537', _2=3.688, _3=3.498, _4=3.6552043117745754, _5=1000.0, _6=1000.0, _7=1000.0),\n",
       " Row(_1=u'LMGHP1S50H1000951', _2=4.199, _3=3.583, _4=3.9384253291203026, _5=1000.0, _6=1000.0, _7=1000.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultDf.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE TABLE guobiao_tsp_tbls.guobiao_vehicle_filter\n",
      "              SELECT * FROM result_Df\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "# spark location on namenode server\n",
    "findspark.init(\"/usr/hdp/current/spark2-client\")\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAll([('spark.app.name', 'guobiao_tsp_tbls.guobiao_vehilce_filter'), # App Name\n",
    "    ('spark.master', 'yarn'),              # spark run mode: locally or remotely\n",
    "    ('spark.submit.deployMode', 'client'), # deploy in yarn-client or yarn-cluster\n",
    "    ('spark.executor.memory', '8g'),       # memory allocated for each executor\n",
    "    ('spark.executor.cores', '3'),         # number of cores for each executor\n",
    "    ('spark.executor.instances', '1'),    # number of executors in total\n",
    "    ('spark.driver.maxResultSize', '5g'), # Result size is large, need to increase from default of 1g\n",
    "    ('spark.yarn.am.memory', '10g')])       # memory for spark driver (application master)\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hc = HiveContext(sc)\n",
    "resultDf.registerTempTable('result_Df')\n",
    "sql_cmd = \"\"\"INSERT OVERWRITE TABLE guobiao_tsp_tbls.guobiao_vehicle_filter\n",
    "              SELECT * FROM result_Df\"\"\"\n",
    "print(sql_cmd)\n",
    "hc.sql(sql_cmd).show()\n",
    "sc.stop()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
